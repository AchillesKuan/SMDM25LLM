{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9972d920-db98-4a8b-8edb-d3b1699e8614",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import os, re\n",
    "from time import ctime\n",
    "import time, math\n",
    "from pathlib import Path\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch\n",
    "import bitsandbytes as bnb\n",
    "from transformers import BitsAndBytesConfig\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "def smm_fewshot(model_name, readfile_name, savefile_name):\n",
    "    qconfig = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, local_files_only=True)\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            device_map=\"cuda\",\n",
    "            # torch_dtype=torch.float16,\n",
    "            quantization_config=qconfig,\n",
    "        )\n",
    "\n",
    "    prompt_template = \"\"\"\n",
    "    You are given clinical note and your task is to determine if a diagnosis of smoldering multiple myeloma was explicitly stated in the given note. The diagnosis must be unambiguous and must exactly mention \"smoldering\" or \"smouldering\" or \"asymptomatic\" or \"indolent\" or \"smm\". \n",
    "    Your responses should be either \"Yes\" or \"No\". Do not respond any texts other than \"Yes\" or \"No\".\n",
    "    \n",
    "    Follow these guidelines:\n",
    "    To identify the explicit diagnosis of smoldering multiple myeloma in clinical notes, you should look for phrases or terms exactly stating \"smoldering\" or \"asymptomatic\" or \"indolent\" or \"smm\". Mention of relevant lab results or treatments alone does not qualify as an explicit diagnosis. Additionally:\n",
    "    Avoid mentions where smoldering myeloma is considered only as a suspicion, a concern, or a differential diagnosis.\n",
    "    A history of smoldering myeloma should not be identified as a current diagnosis.\n",
    "    A diagnosis of MGUS or monoclonal gammopathy or plasma cell dyscrasia or active myeloma should not be mistaken for smoldering myeloma.\n",
    "    Avoid mentions where \"vs\" or \"versus\" is close to smoldering multiple myeloma.\n",
    "    \n",
    "    Here is the the clinical note: {document}\n",
    "    \"\"\" \n",
    "    \n",
    "    def llm_SMDMmodel(user_query):\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are an AI assistant.\"},\n",
    "        ]\n",
    "    \n",
    "        messages.extend([{\"role\":\"user\",\"content\":user_query}])\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "        \n",
    "        generated_ids = model.generate(\n",
    "            **model_inputs,\n",
    "            max_new_tokens=20, \n",
    "            temperature=0.001\n",
    "        )\n",
    "        generated_ids = [\n",
    "            output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "        ]\n",
    "    \n",
    "        response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "        del model_inputs \n",
    "        del generated_ids\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        return response\n",
    "\n",
    "    df = pd.read_json(readfile_name)\n",
    "    Npatient = len(df)\n",
    "    # df2 = df.sample(frac=1, random_state=85).reset_index(drop=False).head(Npatient)\n",
    "    df2 = df \n",
    "    \n",
    "    resulttext = []\n",
    "    Tstart = time.time() \n",
    "    for i in range(Npatient):\n",
    "        x = df2.reportText.to_list()[i]\n",
    "        user_query = prompt_template.format(document=x)\n",
    "        answer = llm_SMDMmodel(user_query)\n",
    "        resulttext.append(answer)\n",
    "        print('Note '+str(i)+' done!')\n",
    "    Tend = time.time() \n",
    "    print('==== %s second =====' %(Tend-Tstart)) #('Duration:{}'.format(Tend-Tstart))\n",
    "    \n",
    "    neg_words = ['no','not','cannot']#,'no,','no**','not','non','negative','suspect','might','likely']\n",
    "    binaryresult = []\n",
    "    for textid in range(len(resulttext)):\n",
    "        if any(word in resulttext[textid][0:2].lower() for word in neg_words): #any(word in resulttext[textid].lower().split() for word in neg_words):\n",
    "            binaryresult.append(0)\n",
    "        else:\n",
    "            binaryresult.append(1)\n",
    "    \n",
    "    dfsavefile = pd.concat([df2.PatientSSN, df2.EpisodeBeginDate, pd.DataFrame({'Output':resulttext})], axis=1)\n",
    "    dfsavefile.to_csv(savefile_name)\n",
    "    #print(dfsavefile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7516ae89-7041-4f85-aaf9-581bd5282967",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \".\\Llama-3.1-8B-Instruct\"\n",
    "readfile_name = r\".\\testingnotes_final.json\" \n",
    "savefile_name = \".\\llama8b\\Llama-8B-fewshot-smm-final.csv\"\n",
    "\n",
    "smm_fewshot(model_name, readfile_name, savefile_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
